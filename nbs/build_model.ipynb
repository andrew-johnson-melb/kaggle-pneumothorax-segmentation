{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pneumothorax segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! git remote add origin https://github.com/andrew-johnson-melb/kaggle-pneumothorax-segmentation.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install albumentations\n",
    "SRC_FILES = '/home/ec2-user/SageMaker/seg_project/src'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from glob import glob\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "sys.path.append(SRC_FILES)\n",
    "\n",
    "from utils import *\n",
    "from unet import UNet\n",
    "from transforms import get_transforms\n",
    "from trainer import train_one_epoch, evaluate\n",
    "from dataset import PneumothoraxDataset\n",
    "from vis import show_batch, compare_masks\n",
    "from loss import MixedLoss, dice_loss, dice_metric, MetricCollector\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LABELLED_DATA = '/home/ec2-user/SageMaker/seg_project/data/preprocessed_data/train/size-512/'\n",
    "LABELS_MASKS = '/home/ec2-user/SageMaker/seg_project/data/preprocessed_data/train-masks/size-512/'\n",
    "TEST_DATA = '/home/ec2-user/SageMaker/seg_project/data/preprocessed_data/test/size-512/'\n",
    "META_DATA_DIR = '/home/ec2-user/SageMaker/seg_project/data/meta_data_siim.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Constructing training and testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load the meta data which contains file names, labels, and patient info\n",
    "then split the dataframe into the labelled data (for development) and the test data used for Kaggles validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_data_df = pd.read_csv(META_DATA_DIR, index_col=0)\n",
    "labelled_df_set = meta_data_df[meta_data_df.train_set]\n",
    "test_df_set = meta_data_df[~meta_data_df.train_set]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Add the full paths to the masks and images, these paths \n",
    "will be used by the dataloader to read the file from disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df_set['images'] = TEST_DATA + test_df_set.file_name + '.png'\n",
    "labelled_df_set['images'] = LABELLED_DATA + labelled_df_set.file_name + '.png'\n",
    "labelled_df_set['masks'] = LABELS_MASKS + labelled_df_set.file_name + '.png'\n",
    "labelled_df_set.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Split the data randomly into a train and test set.\n",
    "Cross validation could be used here to get a more\n",
    "accurate measure of the generalisation of the model but for speed of development we will use a simple random split. Ideally the \n",
    "train/val split would be strafied by the positive class to ensure the validation set has a reasonble number of positive classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set_df, val_set_df = train_dev_split(labelled_df_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset for the training and validation data\n",
    "aug_training, aug_validation = get_transforms()\n",
    "train_dataset = PneumothoraxDataset(files_df=train_set_df, labelled=True, transform=aug_training)\n",
    "val_dataset = PneumothoraxDataset(files_df=val_set_df, labelled=True, transform=aug_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Configs = namedtuple('TrainingConfigs', ['batch_size', 'ratio_pos_neg_sample', 'lr', 'num_epochs'])\n",
    "configs = Configs(batch_size=8, ratio_pos_neg_sample=5, lr=0.00005, num_epochs=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Class imbalance\n",
    "\n",
    "Only 20% of the samples contain the positive class label (indicating Pneumothorax)\n",
    "This imbalance will cause issues when training the model: the model is only \n",
    "predicting zero for the entire region. To counter this we can increase the \n",
    "frequency at which the positive samples are drawn. We do this using \n",
    "the pytorch WeightedRandomSampler. We will use the labels contained \n",
    "in the meta data to construct a vector of weights which the WeightedRandomSampler\n",
    "will use to sample the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set_df = gen_upsampling_weights(train_set_df, ratio_pos_neg=configs.ratio_pos_neg_sample)\n",
    "weighted_sampler = WeightedRandomSampler(weights=train_set_df.weights.values, num_samples=train_set_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=configs.batch_size, num_workers=6, sampler=weighted_sampler)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=8, num_workers=6, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Visualize a batch of training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be very helpful to inspect the transformed data and (normalization aside) exactly the data\n",
    "# that is going into training the model. \n",
    "\n",
    "_ = show_batch(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesing the model outputs and shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some sample data\n",
    "input_, target = get_sample(train_dataloader)\n",
    "\n",
    "# Create model\n",
    "unet = UNet()\n",
    "output = unet(input_)\n",
    "output = output.squeeze()\n",
    "\n",
    "print(f'Input Shape  = {input_.shape}')\n",
    "print(f'Target (y) shape = {target.shape}')\n",
    "print(f\"Output shape = {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lets have a quick look at the model prediction and the mask values.\n",
    "for i in range(1):\n",
    "    im, t = output[i], target[i]\n",
    "    print(f'Best Dice = {dice_loss(im.float(), t.float())}')\n",
    "    print(f'Best Dice = {dice_metric(im.float(), t.float())}')\n",
    "    print(f'Best BCE = {bce_loss(im.float(), t.float())}')\n",
    "    compare_masks(im,t, label=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the sizes \n",
    "print_model_sizes(unet, input_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_epochs = configs.num_epochs\n",
    "\n",
    "model = UNet(freeze_encoder=True)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=configs.lr)\n",
    "\n",
    "# Create a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = MixedLoss(10.0,2.0)\n",
    "validation_losses = {'dice_loss' : dice_loss, 'mixed_loss': loss_fn, 'dice_metric': dice_metric} \n",
    "val_metric_collector = MetricCollector(validation_losses, set_label='validation')\n",
    "train_metric_collector = MetricCollector(validation_losses, set_label='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Random model loss validation')\n",
    "evaluate(model, val_dataloader, device, val_metric_collector, epoch=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch_idx in range(5):\n",
    "    train_one_epoch(model, optimizer, loss_fn, train_dataloader, device , epoch_idx, train_metric_collector)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, val_dataloader, device, val_metric_collector, epoch_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
